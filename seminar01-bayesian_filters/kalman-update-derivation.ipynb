{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## О том как выводить Калмана\n",
    "\n",
    "Имеем произведение двух экспонент, показатели просуммируем и назовем $J_k$\n",
    "\n",
    "$p(x_k \\vert z_k u_k) = \\eta exp \\left( J_k \\right)$\n",
    "\n",
    "$J_k = \\frac{1}{2} (z_k - C_k x_k)^T Q_k^-(z_k - C_k x_k) + \\frac{1}{2} (x_k - \\hat{\\mu_k})^T \\hat{\\Sigma_k}^-(x_k - \\hat{\\mu_k})$\n",
    "\n",
    "Понятно что это квадратичная форма по $x_k$\n",
    "\n",
    "Минимум у нее в среднем, а дисперсия - равна второй производной\n",
    "\n",
    "$\\frac{\\partial J_k}{\\partial x_k} = -C_k^T Q_k^- (z_k - C_k x_k) + \\hat{\\Sigma_k}^-(x_k - \\hat{\\mu_k})$\n",
    "\n",
    "$\\frac{\\partial^2 J_k}{\\partial^2 x_k} = C_k^T Q_k^- C_k + \\hat{\\Sigma_k}^-$\n",
    "\n",
    "С дисперсией ($\\Sigma_k$) разобрались $\\Sigma_k = (C_k^T Q_k^- C_k + \\hat{\\Sigma_k}^-)^-$\n",
    "\n",
    "Среднее (назовем его $\\mu_k$) дается нулем производной\n",
    "\n",
    "$\\hat{\\Sigma_k}^-(\\mu_k - \\hat{\\mu_k}) = C_k^T Q_k^- (z_k - C_k \\mu_k)$\n",
    "\n",
    "$C_k^T Q_k^- (z_k - C_k \\mu_k) = C_k^T Q_k^- (z_k - C_k \\mu_k + C_k \\hat{\\mu_k} - C_k \\hat {\\mu_k}) = C_k^T Q_k^- (z_k - C_k \\hat{\\mu_k}) - C_k^T Q_k^-C_k(\\mu_k - \\hat {\\mu_k})$\n",
    "\n",
    "Подставим обратно и перенесем торое слагаемое налево\n",
    "\n",
    "$(C_k^T Q_k^-C_k + \\hat{\\Sigma_k}^-)(\\mu_k - \\hat{\\mu_k}) = C_k^T Q_k^- (z_k - C_k \\hat{\\mu_k})$\n",
    "\n",
    "$\\Sigma_k^- (\\mu_k - \\hat{\\mu_k}) = C_k^T Q_k^- (z_k - C_k \\hat{\\mu_k})$\n",
    "\n",
    "$\\mu_k = \\Sigma_k C_k^T Q_k^- (z_k - C_k \\hat{\\mu_k}) + \\hat{\\mu_k}$\n",
    "\n",
    "Назовем $K_k = \\Sigma_k C_k^T Q_k^-$\n",
    "\n",
    "$\\mu_k = \\hat{\\mu_k} + K_k (z_k - C_k \\hat{\\mu_k})$\n",
    "\n",
    "\n",
    "В принципе это уже ответ.\n",
    "\n",
    "Еще немного матричной магии для оптимизации вычислений, даст более удобный результат\n",
    "\n",
    "$K_k = \\hat{\\Sigma_k} C_k^T (C_k \\hat{\\Sigma_k} C_k^T + Q_k)^-$\n",
    "\n",
    "$\\mu_k = \\hat{\\mu_k} +K_k (z_k - C_k \\hat{\\mu^k})$\n",
    "\n",
    "$\\Sigma_k = (I - K_k C_k) \\hat{\\Sigma_k}$\n",
    "\n",
    "Такая форма удобнее, т.к. тут обращается матрица размерности наблюдений, что зачастую меньше размерности состояния"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Об оптимальных свойствах Калмана\n",
    "\n",
    "Пусть до получения измерения мы оценили вектор состояния как $x_{k \\vert k-1} \\sim N(\\hat{\\mu_k}, \\hat{\\Sigma_k})$\n",
    "\n",
    "$x_k \\sim N(\\mu_k, \\Sigma_k)$ и положим то $\\mu_k = \\hat{\\mu_k} + K_k(z_k - C_k \\hat{\\mu_k})$ это соответствует нашей интуиции о том, что состояние должно быть где-то между измерением и предсказанием.\n",
    "\n",
    "Будем искать $K_k$ с тем чтобы минимизировать дисперсию  $x_k$, что эквивалентно минимизировать след $\\Sigma_k$\n",
    "\n",
    "$\\Sigma_k = cov(x_k - \\mu_k) = cov(x_k - \\hat{\\mu_k} - K_k(z_k - C_k \\hat{\\mu_k}))$\n",
    "\n",
    "$= cov(x_k - \\hat{\\mu_k} - K_k z_k + K_k C_k \\hat{\\mu_k})$\n",
    "\n",
    "$= cov(x_k - \\hat{\\mu_k} - K_k C_k x_k - K_k \\delta_k + K_k C_k \\hat{\\mu_k})$\n",
    "\n",
    "Шум измерения независим с $x_k$ и обладает ковариацией $Q_k$\n",
    "\n",
    "$= cov(x_k - \\hat{\\mu_k} - K_k C_k x_k + K_k C_k \\hat{\\mu_k}) + K_k Q_k K_k^T$\n",
    "\n",
    "$= cov((x_k - \\hat{\\mu_k}) (I - K_k C_k)) + K_k Q_k K_k^T$\n",
    "\n",
    "$= (I - K_k C_k) \\hat{\\Sigma_k} (I - K_k C_k)^T + K_k Q_k K_k^T$\n",
    "\n",
    "$= \\hat{\\Sigma_k} - K_k C_k \\hat{\\Sigma_k} - \\hat{\\Sigma_k} C_k^T K_k^T + K_k (Q_k + C_k \\hat{\\Sigma_k} C_k^T) K_k^T$\n",
    "\n",
    "$S_k = (Q_k + C_k \\hat{\\Sigma_k} C_k^T)$ - ковариационная матрица шума измерения\n",
    "\n",
    "$= \\hat{\\Sigma_k} - K_k C_k \\hat{\\Sigma_k} - \\hat{\\Sigma_k} C_k^T K_k^T + K_k S_k K_k^T$\n",
    "\n",
    "$= \\hat{\\Sigma_k} + (K_k - \\hat{\\Sigma_k} C_k^T S_k^-) S_k (K_k - \\hat{\\Sigma_k} C_k^T S_k^-)^T - \\hat{\\Sigma_k} C_k^T S_k^- C_k \\hat{\\Sigma_k}$ (тут я вовсю пользуюсь тем что раз $S_k$ ковариационная матрица, то $S_k = S_k^T$)\n",
    "\n",
    "$= \\left[ \\hat{\\Sigma_k} - \\hat{\\Sigma_k} C_k^T S_k^- C_k \\hat{\\Sigma_k} \\right] + \\left[ (K_k - \\hat{\\Sigma_k} C_k^T S_k^-) S_k (K_k - \\hat{\\Sigma_k} C_k^T S_k^-)^T \\right]$\n",
    "\n",
    "Мы хотим минимизировать след этой матрицы по $K_k$, при этом только второе слагаемое зависит от $K_k$ кроме того видно, что оно является какой-то матрицей ковариации, а значит его след в лучшем случае может быть равен нулю, т.е. оптимальное $K_k$ обнуляет это слагаемое\n",
    "\n",
    "$K_k = \\hat{\\Sigma_k} C_k^T S_k^- = \\hat{\\Sigma_k} C_k^T (C_k \\hat{\\Sigma_k} C_k^T + Q_k)^-$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
